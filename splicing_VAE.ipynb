{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jaxlib\n",
    "\n",
    "print(\"jax version:\", jax.__version__)\n",
    "print(\"jaxlib version:\", jaxlib.__version__)\n",
    "\n",
    "import scvi\n",
    "import h5py\n",
    "import anndata as ad\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim import Adam\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "#Read in our AnnData\n",
    "atse_anndata = ad.read_h5ad('/Users/smriti/documents/Research_Knowles_Lab/multivi_tools_splicing/ann_data/ATSE_Anndata_Object_BRAIN_only_20241105_wLeafletFAPSIs.h5ad', backed = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atse_anndata.layers['junc_ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AnnDataDataset Class helps to store layers as a dictionary that contain tensors for each layer so that we can batch them. \n",
    "class AnnDataDataset(Dataset):\n",
    "    def __init__(self, layer_tensors):\n",
    "        self.layer_tensors = layer_tensors\n",
    "        self.num_samples = list(layer_tensors.values())[0].shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {layer_key: tensor[idx] for layer_key, tensor in self.layer_tensors.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder Class - takes inputs and outputs latent variables mean and variance\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, num_hidden_layers, num_hidden_units, latent_dim, dropout_rate = 0.0):\n",
    "        super().__init__()\n",
    "        #consists of 1 input layer, then a series of hidden layers (which can be defined in the params), and 1 output layer\n",
    "        #takes input and outputs TWO latent representation parameters with latent_dim number of dimensions, means and variances for our Normal Distribution\n",
    "        #latent representation\n",
    "\n",
    "        #input layer includes relu and dropout\n",
    "        self.input = nn.Sequential(nn.Linear(input_dim, num_hidden_units[0]), nn.ReLU(), nn.Dropout(dropout_rate))\n",
    "        self.hidden_layers = []\n",
    "        #adding linear layers, each with relu and dropout. dimensions are defined in num_hidden_units\n",
    "        for i in range (num_hidden_layers):\n",
    "            self.hidden_layers.append(nn.Sequential(nn.Linear(num_hidden_units[i], num_hidden_units[i+1]), nn.ReLU(), nn.Dropout(dropout_rate)))\n",
    "        #we output the two different latent rep parameters here. \n",
    "        self.output_means = nn.Linear(num_hidden_units[len(num_hidden_units) - 1], latent_dim)\n",
    "        self.output_log_vars = nn.Linear(num_hidden_units[len(num_hidden_units) - 1], latent_dim)\n",
    "\n",
    "    #forward pass through all our layers\n",
    "    def forward(self, x):\n",
    "        x = self.input(x)\n",
    "        for layer in self.hidden_layers:\n",
    "             x = layer(x)\n",
    "        means = self.output_means(x)\n",
    "        log_vars = self.output_log_vars(x)\n",
    "        return means, log_vars\n",
    "\n",
    "#Decoder Class takes in reparametrized latent representation (z) and creates a logit reconstruction\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim, num_hidden_layers, num_hidden_units, output_dim, dropout_rate = 0.0):\n",
    "        super().__init__()\n",
    "        #similar to encoder, except input dim = latent dim from earlier\n",
    "        self.input = nn.Sequential(nn.Linear(z_dim, num_hidden_units[-1]), nn.ReLU(), nn.Dropout(dropout_rate))\n",
    "        self.hidden_layers = []\n",
    "        #in same way as in encoder we add the hidden layers except in reverse order for the dimensions\n",
    "        for i in reversed(range(num_hidden_layers)):\n",
    "            self.hidden_layers.append(nn.Sequential(nn.Linear(num_hidden_units[i+1], num_hidden_units[i]), nn.ReLU(), nn.Dropout(dropout_rate)))\n",
    "        #output a raw logit representing the reconstruction\n",
    "        self.output = nn.Linear(num_hidden_units[0], output_dim)\n",
    "\n",
    "    #forward pass through all our layers\n",
    "    def forward(self, x):\n",
    "        x = self.input(x)\n",
    "        for layer in self.hidden_layers:\n",
    "             x = layer(x)\n",
    "        reconstruction = self.output(x)\n",
    "        return reconstruction\n",
    "\n",
    "#Binomial loss function includes reconstruction loss and KL divergence\n",
    "def binomial_loss_function(reconstruction, junction_counts, mean, log_vars, n_cluster_counts, eps=1e-04):\n",
    "    log1p_exp_logits = torch.logaddexp(torch.zeros_like(reconstruction), reconstruction) #perturb the logits a bit with an epsilon value\n",
    "    loglik = (junction_counts * reconstruction) - (n_cluster_counts * log1p_exp_logits) #get the log likelihood\n",
    "    reconstruction_loss = -loglik.mean() #binomial loss\n",
    "    kl_divergence = -0.5 * torch.sum(1 + log_vars - mean.pow(2) - log_vars.exp()) #with respect to N(0,I) prior, as MultiVI does\n",
    "    total_loss = reconstruction_loss + kl_divergence\n",
    "    return total_loss\n",
    "\n",
    "#Takes the ATSE AnnData and convert it into a tensor \n",
    "def construct_input_dataloaders(atse_anndata, batch_size):\n",
    "    layer_tensors = {\n",
    "        layer_key: torch.tensor(atse_anndata.layers[layer_key].toarray() if issparse(atse_anndata.layers[layer_key]) else atse_anndata.layers[layer_key], dtype=torch.float32)\n",
    "        for layer_key in list(atse_anndata.layers.keys())[:3] #dictionary of PyTorch tensors that are derived from the first three layers of the AnnData object\n",
    "    } #if sparse matrix, then we convert to a numpy array, and we also make sure to cast it to float32 or else pytorch gets mad at me\n",
    "    dataset = AnnDataDataset(layer_tensors) #put it into our new dataset class defined earlier\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True) #send it to our dataloader and we're off!\n",
    "    #will need to implement validation set and test set probably so I can do early stopping and do some umaps to visualize how the cells are in latent space\n",
    "    return dataloader\n",
    "\n",
    "#Our main VAE model! Takes inputs, encodes them as latent parameters (mean, var), reparametrizes them (z), then decodes them as logit reconstructions\n",
    "#loss is based on how well z can be used to \"reconstruct\" the junction counts, given the ATSE counts. in this way, z is sort of like a denoised representation\n",
    "#of the junction usage ratios (?)\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, num_hidden_layers, num_hidden_units, latent_dim, output_dim, dropout_rate = 0.0):\n",
    "        super().__init__()\n",
    "        #creating our encoder + decoder given our VAE parameters. \n",
    "        self.encoder = Encoder(input_dim, num_hidden_layers, num_hidden_units, latent_dim, dropout_rate)\n",
    "        self.decoder = Decoder(latent_dim, num_hidden_layers, num_hidden_units, output_dim, dropout_rate)\n",
    "    \n",
    "    #reparametrization trick. typically, we'd need to draw z directly from the MVN to sample our latent variable\n",
    "    #as z = N(mean, var)\n",
    "    #but this makes it hard to calculate the gradient later because its stochastic.\n",
    "    #so instead, we represent z as z=mean+ stdâ‹…eps where \n",
    "    #our epsilon is noise to introduce a bit of variability \n",
    "    def reparametrize(self, mean, log_vars):\n",
    "        std = torch.exp(0.5 * log_vars) \n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std  \n",
    "    \n",
    "    #forward pass through our layers\n",
    "    def forward(self, x):\n",
    "        mean, log_vars = self.encoder(x) #get the mean and var from the encoder\n",
    "        z = self.reparametrize(mean, log_vars) #reparametrize to get our z (latent representation)\n",
    "        reconstruction = self.decoder(z) #feed this latent rep directly into our decoder to get our reconstructed logit\n",
    "\n",
    "        reconstruction = reconstruction.to(torch.float32) #make sure its a float!\n",
    "        mean = mean.to(torch.float32) #make this this is also a float (pytorch kept yelling at me)\n",
    "        log_vars = log_vars.to(torch.float32)\n",
    "        \n",
    "        return reconstruction, mean, log_vars\n",
    "    \n",
    "    #finally, we train our model!\n",
    "    def train_model(self, train_dataloader, num_epochs, learning_rate):\n",
    "        print(\"Beginning Training\")\n",
    "        #using adam optimizer\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        train_loss = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0\n",
    "            self.train() #putting our model in train mode\n",
    "            for batch in train_dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                reconstruction, mean, log_vars = self.forward(batch[\"junc_ratio\"]) #calling forward on our junction ratios from the batch\n",
    "                loss = binomial_loss_function(reconstruction, batch[\"cell_by_junction_matrix\"], mean, log_vars, batch[\"cell_by_cluster_matrix\"]) #getting our\n",
    "                #loss, giving the loss function all necessary parameters including the junction and cluster counts\n",
    "                loss.backward() #backward pass\n",
    "                epoch_loss += loss.item()\n",
    "                optimizer.step()\n",
    "            print(f\"Epoch {epoch+1} of {num_epochs}; Train Loss = {epoch_loss/len(train_dataloader)}\")\n",
    "            train_loss += epoch_loss/len(train_dataloader)\n",
    "        return train_loss/num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Karin's parameters from her VAE for now, will change once I run this on the cluster.\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "INPUT_DIM = atse_anndata.var.shape[0]\n",
    "NUM_HIDDEN_LAYERS = 1\n",
    "HIDDEN_DIMS = [128, 64] \n",
    "LATENT_DIM = 20\n",
    "OUTPUT_DIM = INPUT_DIM \n",
    "#getting our dataloader from the atse anndata, given our batch size. \n",
    "dataloader = construct_input_dataloaders(atse_anndata, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training call! puts it on our GPU if available\n",
    "model = VAE(INPUT_DIM, NUM_HIDDEN_LAYERS, HIDDEN_DIMS, LATENT_DIM, OUTPUT_DIM)\n",
    "if USE_CUDA:\n",
    "    model.cuda()\n",
    "\n",
    "model.train_model(dataloader, NUM_EPOCHS, learning_rate = 1e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scvi-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
