{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCpliceVAE Interactive Test Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 1. CLASS DEFINITIONS HERE AND IMPORTS\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import anndata as ad\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import csr_matrix, issparse\n",
    "from collections import defaultdict\n",
    "import json \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import anndata as ad\n",
    "import os\n",
    "import matplotlib.pyplot as plt  # For plotting results\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ensure the path to your module is in sys.path\n",
    "# Modify this to point to where your modules are located\n",
    "module_path = '/gpfs/commons/home/kisaev/multivi_tools_splicing/src/SCplice_vae'  # Change this to your module path\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from dataloaders import * \n",
    "\n",
    "from partial_vae import (\n",
    "    PartialEncoder, \n",
    "    PartialDecoder, \n",
    "    PartialVAE, \n",
    "    binomial_loss_function, \n",
    "    beta_binomial_loss_function,\n",
    ")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    \n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load Some Simulated Data for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AnnData Summary:\n",
      "AnnData object with n_obs × n_vars = 19942 × 9798\n",
      "    obs: 'cell_id', 'age', 'batch', 'cell_ontology_class', 'method', 'mouse.id', 'sex', 'tissue', 'old_cell_id_index', 'cell_clean', 'cell_id_index', 'subtissue_clean', 'cell_type_grouped', 'cell_type'\n",
      "    var: 'junction_id', 'event_id', 'splice_motif', 'label_5_prime', 'label_3_prime', 'annotation_status', 'gene_name', 'gene_id', 'num_junctions', 'position_off_5_prime', 'position_off_3_prime', 'CountJuncs', 'non_zero_count_cells', 'non_zero_cell_prop', 'annotation_status_score', 'non_zero_cell_prop_score', 'splice_motif_score', 'junction_id_index', 'chr', 'start', 'end', 'index', '0', '1', '2', '3', '4', '5', '6', '7', '8', 'sample_label', 'difference', 'true_label'\n",
      "    uns: 'age_colors', 'cell_type_colors', 'neighbors', 'pca_explained_variance_ratio', 'tissue_colors', 'umap'\n",
      "    obsm: 'X_leafletFA', 'X_pca', 'X_umap', 'phi_init_100_waypoints', 'phi_init_30_waypoints'\n",
      "    varm: 'psi_init_100_waypoints', 'psi_init_30_waypoints'\n",
      "    layers: 'Cluster_Counts', 'Junction_Counts', 'cell_by_cluster_matrix', 'cell_by_junction_matrix', 'imputed_PSI', 'junc_ratio'\n",
      "    obsp: 'connectivities', 'distances'\n",
      "\n",
      "Layers:\n",
      "  Cluster_Counts: <class 'scipy.sparse._csr.csr_matrix'>, shape (19942, 9798)\n",
      "  Junction_Counts: <class 'scipy.sparse._csr.csr_matrix'>, shape (19942, 9798)\n",
      "  cell_by_cluster_matrix: <class 'scipy.sparse._csr.csr_matrix'>, shape (19942, 9798)\n",
      "  cell_by_junction_matrix: <class 'scipy.sparse._csr.csr_matrix'>, shape (19942, 9798)\n",
      "  imputed_PSI: <class 'numpy.ndarray'>, shape (19942, 9798)\n",
      "  junc_ratio: <class 'numpy.ndarray'>, shape (19942, 9798)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/commons/home/kisaev/miniconda3/envs/LeafletSC/lib/python3.10/site-packages/anndata/_core/aligned_df.py:68: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Load AnnData object with simulated single cell splicing data \n",
    "# Create synthetic data\n",
    "adata = ad.read_h5ad('/gpfs/commons/groups/knowles_lab/Karin/TMS_MODELING/DATA_FILES/SIMULATED/simulated_data_2025-03-12.h5ad')\n",
    "\n",
    "# Basic info about the data\n",
    "print(\"\\nAnnData Summary:\")\n",
    "print(adata)\n",
    "print(\"\\nLayers:\")\n",
    "for layer_name, layer in adata.layers.items():\n",
    "    print(f\"  {layer_name}: {type(layer)}, shape {layer.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep the layers you're interested in\n",
    "trimmed_layers = {\n",
    "    key: adata.layers[key]\n",
    "    for key in [\"junc_ratio\", \"cell_by_cluster_matrix\", \"cell_by_junction_matrix\"]\n",
    "}\n",
    "\n",
    "# Create trimmed AnnData\n",
    "adata_trimmed = ad.AnnData(\n",
    "    X=None,  # Don't include full X matrix\n",
    "    obs=adata.obs.copy(),\n",
    "    var=adata.var.copy(),\n",
    "    layers=trimmed_layers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 13959, Batches: 28\n",
      "Validation samples: 5983, Batches: 12\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "X_LAYER_NAME = 'junc_ratio'\n",
    "JUNCTION_COUNTS_LAYER_NAME = 'cell_by_junction_matrix'\n",
    "CLUSTER_COUNTS_LAYER_NAME = 'cell_by_cluster_matrix'\n",
    "BATCH_SIZE = 512 # Adjust as needed\n",
    "NUM_WORKERS = 2 # Adjust based on your system\n",
    "\n",
    "# --- Create Train/Validation Split ---\n",
    "all_indices = np.arange(adata_trimmed.n_obs)\n",
    "train_indices, val_indices = train_test_split(all_indices, test_size=0.3, random_state=42) # 10% validation\n",
    "\n",
    "# --- Create Datasets ---\n",
    "train_dataset = AnnDataDataset(\n",
    "    adata_trimmed,\n",
    "    x_layer=X_LAYER_NAME,\n",
    "    junction_counts_layer=JUNCTION_COUNTS_LAYER_NAME,\n",
    "    cluster_counts_layer=CLUSTER_COUNTS_LAYER_NAME,\n",
    "    obs_indices=train_indices.tolist() # Pass the list of indices for the training set\n",
    ")\n",
    "\n",
    "val_dataset = AnnDataDataset(\n",
    "    adata_trimmed,\n",
    "    x_layer=X_LAYER_NAME,\n",
    "    junction_counts_layer=JUNCTION_COUNTS_LAYER_NAME,\n",
    "    cluster_counts_layer=CLUSTER_COUNTS_LAYER_NAME,\n",
    "    obs_indices=val_indices.tolist() # Pass the list of indices for the validation set\n",
    ")\n",
    "\n",
    "# --- Create DataLoaders ---\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,         # Don't shuffle training data\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,      # Can speed up CPU->GPU transfer\n",
    "    drop_last=False       # Keep the last batch even if smaller\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,        # No need to shuffle validation data\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "# --- Get dataset size and number of batches (for loss function scaling) ---\n",
    "n_train_samples = len(train_dataset)\n",
    "k_train_batches = len(train_loader)\n",
    "n_val_samples = len(val_dataset)\n",
    "k_val_batches = len(val_loader)\n",
    "\n",
    "print(f\"Training samples: {n_train_samples}, Batches: {k_train_batches}\")\n",
    "print(f\"Validation samples: {n_val_samples}, Batches: {k_val_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Dimension (n_vars): 9798\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 2. CONFIGURATION & HYPERPARAMETERS\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Data ---\n",
    "X_LAYER_NAME = 'junc_ratio'\n",
    "JUNCTION_COUNTS_LAYER_NAME = 'cell_by_junction_matrix'\n",
    "CLUSTER_COUNTS_LAYER_NAME = 'cell_by_cluster_matrix'\n",
    "INPUT_DIM = adata_trimmed.n_vars # Get input dimension from data\n",
    "print(f\"Input Dimension (n_vars): {INPUT_DIM}\")\n",
    "\n",
    "# --- Model Architecture ---\n",
    "# INPUT_DIM will be set from data\n",
    "CODE_DIM = 16             # Dimension K for feature embeddings (junction embeddings)\n",
    "H_HIDDEN_DIM = 64         # Hidden dim for encoder's h_layer \n",
    "ENCODER_HIDDEN_DIM = 128  # Hidden dim for encoder's final MLP\n",
    "LATENT_DIM = 10           # Dimension Z for latent space\n",
    "DECODER_HIDDEN_DIM = 128  # Hidden dim for decoder\n",
    "DROPOUT_RATE = 0.01\n",
    "\n",
    "# --- Training ---\n",
    "LOSS_TYPE = 'binomial' # Choose 'binomial' or 'beta_binomial'\n",
    "LEARN_CONCENTRATION = True  # Set True for beta-binomial if you want learnable concentration\n",
    "FIXED_CONCENTRATION = None  # Set to a float (e.g., 10.0) if using beta-binomial with FIXED concentration\n",
    "                            # If set, overrides LEARN_CONCENTRATION=True\n",
    "\n",
    "NUM_EPOCHS = 100          # Max number of epochs\n",
    "LEARNING_RATE = 0.01\n",
    "PATIENCE = 10             # Early stopping patience (epochs)\n",
    "SCHEDULE_STEP_SIZE = 10   # LR scheduler step size\n",
    "SCHEDULE_GAMMA = 0.1      # LR scheduler factor\n",
    "\n",
    "# --- Output & Logging ---\n",
    "OUTPUT_DIR = \"./vae_training_output\" # Directory to save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model initialized:\n",
      "PartialVAE(\n",
      "  (encoder): PartialEncoder(\n",
      "    (h_layer): Sequential(\n",
      "      (0): Linear(in_features=18, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.01, inplace=False)\n",
      "      (3): Linear(in_features=64, out_features=16, bias=True)\n",
      "      (4): ReLU()\n",
      "    )\n",
      "    (encoder_mlp): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.01, inplace=False)\n",
      "      (3): Linear(in_features=128, out_features=20, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (decoder): PartialDecoder(\n",
      "    (z_processor): Sequential(\n",
      "      (0): Linear(in_features=10, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.01, inplace=False)\n",
      "    )\n",
      "    (j_layer): Sequential(\n",
      "      (0): Linear(in_features=145, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.01, inplace=False)\n",
      "      (3): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Using Binomial Loss.\n",
      "Warning: Concentration parameters ignored for binomial loss.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 3. MODEL INITIALIZATION & LOSS FUNCTION SELECTION\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Setup Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Instantiate Model ---\n",
    "# Determine if concentration should be learned based on settings\n",
    "should_learn_concentration = (LOSS_TYPE == 'beta_binomial') and (FIXED_CONCENTRATION is None) and LEARN_CONCENTRATION\n",
    "\n",
    "model = PartialVAE(\n",
    "    input_dim=INPUT_DIM,\n",
    "    code_dim=CODE_DIM,\n",
    "    h_hidden_dim=H_HIDDEN_DIM,\n",
    "    encoder_hidden_dim=ENCODER_HIDDEN_DIM,\n",
    "    latent_dim=LATENT_DIM,\n",
    "    decoder_hidden_dim=DECODER_HIDDEN_DIM,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    learn_concentration=should_learn_concentration # Pass the determined flag\n",
    ")\n",
    "model.to(device)\n",
    "print(\"Model initialized:\")\n",
    "print(model)\n",
    "\n",
    "# --- Choose Loss Function ---\n",
    "if LOSS_TYPE == 'binomial':\n",
    "    chosen_loss_function = binomial_loss_function\n",
    "    print(\"Using Binomial Loss.\")\n",
    "    if FIXED_CONCENTRATION is not None or LEARN_CONCENTRATION:\n",
    "         print(\"Warning: Concentration parameters ignored for binomial loss.\")\n",
    "elif LOSS_TYPE == 'beta_binomial':\n",
    "    chosen_loss_function = beta_binomial_loss_function\n",
    "    print(\"Using Beta-Binomial Loss.\")\n",
    "    if FIXED_CONCENTRATION is not None:\n",
    "        print(f\"Using FIXED concentration: {FIXED_CONCENTRATION}\")\n",
    "    elif should_learn_concentration:\n",
    "        print(\"Using LEARNABLE concentration.\")\n",
    "    else:\n",
    "        print(\"Warning: Beta-binomial selected but no concentration specified (fixed or learnable). Check config.\")\n",
    "else:\n",
    "    raise ValueError(f\"Unknown LOSS_TYPE: '{LOSS_TYPE}'. Choose 'binomial' or 'beta_binomial'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch keys: dict_keys(['x', 'mask', 'junction_counts', 'cluster_counts'])\n",
      "Batch shape: {'x': torch.Size([512, 9798]), 'mask': torch.Size([512, 9798]), 'junction_counts': torch.Size([512, 9798]), 'cluster_counts': torch.Size([512, 9798])}\n"
     ]
    }
   ],
   "source": [
    "# check the first element in the train dataloader \n",
    "for batch in train_loader:\n",
    "    print(\"Batch keys:\", batch.keys())\n",
    "    print(\"Batch shape:\", {k: v.shape for k, v in batch.items()})\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output will be saved to: /gpfs/commons/home/kisaev/multivi_tools_splicing/testing/vae_training_output\n",
      "Beginning training on device: cuda:0\n",
      "Epoch 001/100 | Train Loss: 11405411.6925 | LR: 1.0e-02\n",
      "          | Val Loss:   81754.1829\n",
      "          | Val loss improved (inf -> 81754.1829). Saving model to ./vae_training_output/model/best_model.pth\n",
      "Epoch 002/100 | Train Loss: 245180.5818 | LR: 1.0e-02\n",
      "          | Val Loss:   145214.1185\n",
      "          | Val loss did not improve. Bad epochs: 1/10\n",
      "Epoch 003/100 | Train Loss: 115836.0516 | LR: 1.0e-02\n",
      "          | Val Loss:   117241.7461\n",
      "          | Val loss did not improve. Bad epochs: 2/10\n",
      "Epoch 004/100 | Train Loss: 215245.4170 | LR: 1.0e-02\n",
      "          | Val Loss:   115809.9805\n",
      "          | Val loss did not improve. Bad epochs: 3/10\n",
      "Epoch 005/100 | Train Loss: 102042.7833 | LR: 1.0e-02\n",
      "          | Val Loss:   46610.8962\n",
      "          | Val loss improved (81754.1829 -> 46610.8962). Saving model to ./vae_training_output/model/best_model.pth\n",
      "Epoch 006/100 | Train Loss: 49515.3175 | LR: 1.0e-02\n",
      "          | Val Loss:   42587.7432\n",
      "          | Val loss improved (46610.8962 -> 42587.7432). Saving model to ./vae_training_output/model/best_model.pth\n",
      "Epoch 007/100 | Train Loss: 43509.8883 | LR: 1.0e-02\n",
      "          | Val Loss:   41553.2699\n",
      "          | Val loss improved (42587.7432 -> 41553.2699). Saving model to ./vae_training_output/model/best_model.pth\n",
      "Epoch 008/100 | Train Loss: 42118.4102 | LR: 1.0e-02\n",
      "          | Val Loss:   41080.1055\n",
      "          | Val loss improved (41553.2699 -> 41080.1055). Saving model to ./vae_training_output/model/best_model.pth\n",
      "Epoch 009/100 | Train Loss: 41824.1129 | LR: 1.0e-02\n",
      "          | Val Loss:   40965.8831\n",
      "          | Val loss improved (41080.1055 -> 40965.8831). Saving model to ./vae_training_output/model/best_model.pth\n",
      "Epoch 010/100 | Train Loss: 41578.1957 | LR: 1.0e-03\n",
      "          | Val Loss:   40879.0654\n",
      "          | Val loss improved (40965.8831 -> 40879.0654). Saving model to ./vae_training_output/model/best_model.pth\n",
      "Epoch 011/100 | Train Loss: 41347.2415 | LR: 1.0e-03\n",
      "          | Val Loss:   40523.6882\n",
      "          | Val loss improved (40879.0654 -> 40523.6882). Saving model to ./vae_training_output/model/best_model.pth\n",
      "Epoch 012/100 | Train Loss: 41336.1837 | LR: 1.0e-03\n",
      "          | Val Loss:   40509.5368\n",
      "          | Val loss improved (40523.6882 -> 40509.5368). Saving model to ./vae_training_output/model/best_model.pth\n",
      "Epoch 013/100 | Train Loss: 41242.3082 | LR: 1.0e-03\n",
      "          | Val Loss:   40464.0329\n",
      "          | Val loss improved (40509.5368 -> 40464.0329). Saving model to ./vae_training_output/model/best_model.pth\n",
      "Epoch 014/100 | Train Loss: 41173.8470 | LR: 1.0e-03\n",
      "          | Val Loss:   40459.5046\n",
      "          | Val loss improved (40464.0329 -> 40459.5046). Saving model to ./vae_training_output/model/best_model.pth\n",
      "Epoch 015/100 | Train Loss: 41108.9050 | LR: 1.0e-03\n",
      "          | Val Loss:   40404.3988\n",
      "          | Val loss improved (40459.5046 -> 40404.3988). Saving model to ./vae_training_output/model/best_model.pth\n",
      "Epoch 016/100 | Train Loss: 41030.0131 | LR: 1.0e-03\n",
      "          | Val Loss:   40362.4202\n",
      "          | Val loss improved (40404.3988 -> 40362.4202). Saving model to ./vae_training_output/model/best_model.pth\n",
      "Epoch 017/100 | Train Loss: 41040.8157 | LR: 1.0e-03\n",
      "          | Val Loss:   40337.7744\n",
      "          | Val loss improved (40362.4202 -> 40337.7744). Saving model to ./vae_training_output/model/best_model.pth\n",
      "Epoch 018/100 | Train Loss: 41042.9452 | LR: 1.0e-03\n",
      "          | Val Loss:   40305.5726\n",
      "          | Val loss improved (40337.7744 -> 40305.5726). Saving model to ./vae_training_output/model/best_model.pth\n",
      "Epoch 019/100 | Train Loss: 40966.6946 | LR: 1.0e-03\n",
      "          | Val Loss:   40273.4352\n",
      "          | Val loss improved (40305.5726 -> 40273.4352). Saving model to ./vae_training_output/model/best_model.pth\n",
      "Epoch 020/100 | Train Loss: 40959.8045 | LR: 1.0e-04\n",
      "          | Val Loss:   40244.4557\n",
      "          | Val loss improved (40273.4352 -> 40244.4557). Saving model to ./vae_training_output/model/best_model.pth\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 4. TRAINING EXECUTION\n",
    "# ==============================================================================\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if OUTPUT_DIR:\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    print(f\"Output will be saved to: {os.path.abspath(OUTPUT_DIR)}\")\n",
    "\n",
    "# --- Start Training ---\n",
    "try:\n",
    "    train_losses, val_losses, epochs_trained = model.train_model(\n",
    "        loss_function=chosen_loss_function,\n",
    "        train_dataloader=train_loader,\n",
    "        val_dataloader=val_loader,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        patience=PATIENCE,\n",
    "        fixed_concentration=FIXED_CONCENTRATION, # Pass the fixed value if set\n",
    "        schedule_step_size=SCHEDULE_STEP_SIZE,\n",
    "        schedule_gamma=SCHEDULE_GAMMA,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        # --- Specify keys matching AnnDataDataset output ---\n",
    "        input_key='x',\n",
    "        mask_key='mask',\n",
    "        junction_counts_key='junction_counts',\n",
    "        cluster_counts_key='cluster_counts'\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during training: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc() # Print detailed traceback\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 5. POST-TRAINING (Example: Plot Losses)\n",
    "# ==============================================================================\n",
    "print(\"\\nTraining complete.\")\n",
    "\n",
    "if train_losses and val_losses:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    epochs = range(1, epochs_trained + 1)\n",
    "    plt.plot(epochs, train_losses, label='Training Loss')\n",
    "    plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('VAE Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    # Save the plot\n",
    "    plot_path = \"training_loss_plot.png\"\n",
    "    if OUTPUT_DIR:\n",
    "        plot_path = os.path.join(OUTPUT_DIR, \"training_loss_plot.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    print(f\"Loss plot saved to {plot_path}\")\n",
    "    plt.show() # Uncomment to display plot interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Select Data for Visualization (e.g., all data or a subset) ---\n",
    "# Using all data here, adjust if needed (e.g., use val_indices from training)\n",
    "indices_to_use = np.arange(adata.n_obs)\n",
    "print(f\"Getting latent representations for {len(indices_to_use)} cells...\")\n",
    "\n",
    "CELL_TYPE_COLUMN = \"cell_type\"\n",
    "\n",
    "# Ensure labels are in a usable format (e.g., strings)\n",
    "cell_labels = adata.obs[CELL_TYPE_COLUMN][indices_to_use].astype(str).values\n",
    "\n",
    "# --- Create Dataset and DataLoader for Inference ---\n",
    "# Ensure AnnDataDataset uses the BOOLEAN mask fix\n",
    "inference_dataset = AnnDataDataset(\n",
    "    adata,\n",
    "    x_layer=X_LAYER_NAME,\n",
    "    junction_counts_layer=JUNCTION_COUNTS_LAYER_NAME, # Still needed by dataset init\n",
    "    cluster_counts_layer=CLUSTER_COUNTS_LAYER_NAME, # Still needed by dataset init\n",
    "    obs_indices=indices_to_use.tolist()\n",
    ")\n",
    "\n",
    "inference_loader = DataLoader(\n",
    "    inference_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False, # DO NOT shuffle for inference if matching labels\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "# --- Iterate and Collect Latent Representations ---\n",
    "latent_reps_list = []\n",
    "with torch.no_grad(): # Ensure no gradients are calculated\n",
    "    for i, batch in enumerate(inference_loader):\n",
    "        print(f\"  Processing batch {i+1}/{len(inference_loader)}...\", end='\\r')\n",
    "        x_batch = batch['x'] # Data loader provides tensors\n",
    "        mask_batch = batch['mask'] # Data loader provides tensors\n",
    "\n",
    "        # Use the model's method (handles device and eval mode)\n",
    "        latent_batch_np = model.get_latent_rep(x_batch, mask_batch)\n",
    "        latent_reps_list.append(latent_batch_np)\n",
    "\n",
    "print(\"\\nFinished collecting latent representations.\")\n",
    "\n",
    "# Concatenate all batch results\n",
    "all_latent_reps = np.concatenate(latent_reps_list, axis=0)\n",
    "print(f\"Shape of collected latent representations: {all_latent_reps.shape}\") # Should be (n_obs, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import anndata as ad\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # For better plotting aesthetics\n",
    "from umap import UMAP # Import UMAP\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "# --- Visualization ---\n",
    "# UMAP parameters (can be tuned)\n",
    "N_NEIGHBORS = 15\n",
    "MIN_DIST = 0.1\n",
    "UMAP_METRIC = 'euclidean' # Distance metric in latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 5. DIMENSIONALITY REDUCTION (UMAP)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Running UMAP for dimensionality reduction...\")\n",
    "reducer = UMAP(\n",
    "    n_components=2,       # Reduce to 2 dimensions for plotting\n",
    "    n_neighbors=N_NEIGHBORS,    # Controls local vs global structure (adjust)\n",
    "    min_dist=MIN_DIST,      # Controls tightness of clusters (adjust)\n",
    "    metric=UMAP_METRIC,     # Distance metric in the latent space\n",
    "    random_state=42       # For reproducibility\n",
    ")\n",
    "\n",
    "embedding_2d = reducer.fit_transform(all_latent_reps)\n",
    "print(f\"Shape of 2D UMAP embedding: {embedding_2d.shape}\") # Should be (n_obs, 2)\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. PLOTTING\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Generating plot...\")\n",
    "\n",
    "# --- Create the plot ---\n",
    "plt.figure(figsize=(12, 10)) # Adjust figure size as needed\n",
    "\n",
    "# Use seaborn for potentially nicer aesthetics and easier legend handling\n",
    "# Choose a suitable palette, 'tab20' works for up to 20 categories, 'viridis'/'plasma' for continuous-like\n",
    "# Adjust 's' for point size, 'alpha' for transparency\n",
    "num_unique_labels = len(np.unique(cell_labels))\n",
    "palette = sns.color_palette('tab20', n_colors=num_unique_labels) # Example palette\n",
    "\n",
    "scatter = sns.scatterplot(\n",
    "    x=embedding_2d[:, 0],\n",
    "    y=embedding_2d[:, 1],\n",
    "    hue=cell_labels,      # Color points by cell type\n",
    "    palette=palette,      # Color map\n",
    "    s=5,                  # Point size\n",
    "    alpha=0.7,            # Point transparency\n",
    "    linewidth=0           # No border around points\n",
    ")\n",
    "\n",
    "# --- Customize plot ---\n",
    "plt.title(f'UMAP Projection of VAE Latent Space (Z={LATENT_DIM})', fontsize=16)\n",
    "plt.xlabel('UMAP 1', fontsize=12)\n",
    "plt.ylabel('UMAP 2', fontsize=12)\n",
    "plt.xticks([]) # Hide axis ticks for cleaner look\n",
    "plt.yticks([])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeafletSC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
